import pandas as pd
from bs4 import BeautifulSoup
import pymysql, calendar, time, json
import requests
from datetime import datetime
from threading import Timer


class DBUpdater:
    def __init__(self):
        """생성자: MariaDB 연결 및 종목코드 딕셔너리 생성"""
        self.conn = pymysql.connect(host='localhost', port=13306, user='root', password='qwaszx2689!', db='Investar', charset='utf8')

        with self.conn.cursor() as curs:
            sql = """
            CREATE TABLE IF NOT EXISTS company_info (
                code VARCHAR(20),
                company VARCHAR(40),
                last_update date,
                PRIMARY KEY (code)
            )default charset = utf8
            """
            curs.execute(sql)
            sql = """
            CREATE TABLE if NOT EXISTS daily_price (
                code VARCHAR(20),
                date DATE,
                open BIGINT(20),
                high BIGINT(20),
                low BIGINT(20),
                close BIGINT(20),
                diff BIGINT(20),
                volume BIGINT(20),
                PRIMARY KEY (code, date)
            )DEFAULT CHARSET = utf8
            """
            curs.execute(sql)
        self.conn.commit()

        self.codes = dict()
        self.update_comp_info()

            
    def __del__ (self):
        """소멸자: MariaDB 연결 해제"""
        self.conn.close()

    def read_krx_code(self):
        """KRX로부터 상장법인목록 파일을 읽어와서 데이터프레임으로 반환"""
        url = 'http://kind.krx.co.kr/corpgeneral/corpList.do?method=download&searchType=13'

        # 상장법인목록.xls 파일을 read_html() 함수로 읽는다.
        krx = pd.read_html(url, header=0)[0]
        # 종목코드 칼럼과 회사명만 남긴다. 데이터프레임에 [[]]을 사용하면 특정 칼럼만 뽑아서 원하는 순서대로 재구성 가능
        krx = krx[['종목코드', '회사명']]
        # 한글 칼럼명을 영문 칼럼명으로 변경한다.
        krx = krx.rename(columns={'종목코드': 'code', '회사명': 'company'})
        # 종목코드 형식을 {:06d} 형식의 문자열로 변경한다.
        krx.code = krx.code.map('{:06d}'.format)

        return krx

    # 일반적으로 테이블에 데이터 행을 삽입하는 데 INSERT INTO 구문을 사용하지만, INSERT INTO 구문 역시 
    # 데이터 행이 테이블에 이미 존재하면 오류가 발생해 프로그램이 종료된다. 
    # 표준 SQL문은 아니지만 마리아디비에서 제공하는 REPLACE INTO 구문을 사용하면, 동일한 데이터 행이 존재하더라도
    # 오류를 발생하지 않고 UPDATE를 수행한다. REPLACE INTO 구문처럼 INSERT와 UPDATE를 합쳐놓은 기능을 UPSERT라고 부르기도한다.
    def update_comp_info(self):
        """종목코드를 company_info 테이블에 업데이트한 후 딕셔너리에 저장"""
        sql = 'SELECT * FROM company_info'
        # company_info 테이블을 read_sql() 함수로 읽는다.
        df = pd.read_sql(sql, self.conn)
        for idx in range(len(df)):
            # 위에서 읽은 데이터프레임을 이용해서 종목코드와 회사명으로 codes 딕셔너리를 만든다.
            self.codes[df['code'].values[idx]] = df['company'].values[idx]
        
        with self.conn.cursor() as curs:
            sql = "SELECT max(last_update) FROM company_info"
            curs.execute(sql)
            # SELECT max() ~구문을 이용해서 DB에서 가장 최근 업데이트 날짜를 가져온다.
            rs = curs.fetchone()
            today = datetime.today().strftime('%Y-%m-%d')

            # 위에서 구한 날짜가 존재하지 않거나 오늘보다 오래된 경우에만 업데이트한다.
            if rs[0] == None or rs[0].strftime('%Y-%m-%d') < today:
                # KRX 상장기업 목록파일을 읽어서 krx 데이터프레임에 저장한다.
                krx = self.read_krx_code()
                for idx in range(len(krx)):
                    code = krx.code.values[idx]
                    company = krx.company.values[idx]
                    sql = f"REPLACE INTO company_info (code, company, last_update) VALUES ('{code}', '{company}', '{today}')"
                    
                    # REPLACE INTO 구문을 이용해서 "종목코드, 회사명, 오늘날짜" 행을 DB에 저장한다.
                    curs.execute(sql)
                    # codes 딕셔너리에 '키-값'으로 종목코드와 회사명을 추가한다.
                    self.codes[code] = company
                    tmnow = datetime.now().strftime('%Y-%m-%d %H:%M')
                    print(f"[{tmnow}] {idx:04d} REPLACE INTO company_info VALUES ({code}, {company}, {today})")
                    
                    self.conn.commit()
                    print('')


    
    def read_naver(self, code, company, pages_to_fetch):
        """네이버 금융에서 주식 시세를 읽어서 데이터프레임으로 변환"""
    # KRX 종목코드를 DB에 업데이트했으므로 이제 네이버 금융으로부터 모든 종목의 일별 시세 데이터를 스크레이핑하자. 네이버의 시제 페이지를 
    # 스크레이핑하는 코드는 뷰티풀 수프를 설명할 때 사용했던 코드와 같다.
    # 다만 pgRR 클래스의 <td> 태그가 존재하지 않으면 AttributeError가 발생하면서 프로그램이 종료되므로, find() 함수 결과가 None 인 경우에는 다음 종목을 처리하도록 변경

    # try except 예외 처리
    # 다음은 네이버 금융에서 일별 시세를 스크레이피하는 코드다. 뷰티풀 수프로 전체 페이지 수를 구했다면, 팬더스의 read_html() 함수를 이용하여 첫 페이지부터 마지막 페이지까지 순차적으로 
    # 주식 시세 데이터를 읽어와야 한다.

        try: 
            url = f"http://finance.naver.com/item/sise_day.nhn?code={code}"

            with urlopen(url) as doc:
                if doc is None:
                    return None

                html = BeautifulSoup(requests.get(url, headers={'User-agent': 'Mozilla/5.0'}).text, "lxml")
                pgrr = html.find("td", class_="pgRR") 
                if pgrr is None:
                    return None

                s = str(pgrr.a["href"]).split("=")
                # 네이버 금융에서 일별 시세의 마지막 페이지를 구한다.
                lastpage = s[-1]
            
            df = pd.DataFrame()
            # 설정 파일에 설정된 페이지 수(pages_to_fetch)와 위의 페이지 수에서 작은 것을 택한다.
            pages = min(int(lastpage), pages_to_fetch)
            for page in range(1, pages+1):
                pg_url = '{}&page={}'.format(url, page)
                # 일별 시세 페이지를 read_html()로 읽어서 데이터프레임에 추가한다.
                df = df.append(pd.read_html(requests.get(pg_url, headers={'User-agent': 'Mozilla/5.0'}).text)[0])
                tmnow = datetime.now().strftime('%Y-%m-%d %H:%M')
                print('[{}] {} ({}) : {:04d}/{:04d} pages are downloading...'.format(tmnow, company, code, page, pages), end='\r')
            
            # 네이버 금융의 한글 칼럼명을 영문 칼럼명으로 변경한다.
            df = df.rename(columns={'날짜':'date', '종가':'close', '전일비':'diff', '시가':'open', '고가':'high', '저가':'low', '거래량':'volume'})
            df['date'] = df['date'].replace('.', '-')
            df = df.dropna()
            # 마리아디비에서 BIGINT형으로 지정한 칼럼들의 데이터형을 int형으로 변경한다.
            df[['close', 'diff', 'open', 'high', 'low', 'volume']] = df[['close', 'diff', 'open', 'high', 'low', 'volume']].astype(int)
            # 원하는 순서로 칼럼을 재조합하여 데이터프레임을 만든다.
            df = df[['close', 'diff', 'open', 'high', 'low', 'volume']]
        
        except Exception as e:
            print('Exception occured : ', str(e))
            return None
        return df
    
    def replace_into_db(self, df, num, code, company):
        """네이버 금융에서 읽어온 주식 시세를 DB에 REPLACE"""
    # 다음은 read_naver() 메서드로 읽어온 네이버 일별 시세를 DB에 저장하는 replace_into_db 메서드다. 팬더스의 to_sql()를 사용해서 DB에 저장할 수도 
    # 있지만, 그러려면 종목별로 테이블을 구성해야 하고 to_sql() 함수가 데이터를 저장할 때 기존 테이블을 전체적으로 교체하기 때문에 효율적이지 않다.

        with self.conn.cursor() as curs:
            # 인수로 넘겨받은 데이터프레임을 튜플로 순회처리한다.
            for r in df.itertuples():
                sql = f"REPLACE INTO daily_price VALUES ('{code}', "\
                    f"'{r.date}', {r.open}, {r.high}, {r.low}, {r.close}, "\
                    f"{r.diff}, {r.volume})"
                
                # REPLACE INTO 구문으로 daily_price 테이블을 업데이트한다.
                curs.execute(sql)
            # commit() 함수를 호출해 마리아디비에 반영한다.
            self.conn.commit()
            print('[{}] #{:04d} {} ({}) : {} rows > REPLACE INTO daily_'\
                'price [OK]'.format(datetime.now().strftime('%Y-%m-%d'\
                ' %H:%M'), num+1, company, code, len(df)))
    
    def update_daily_price(self, pages_to_fetch):
        """KRX 상장법인의 주식 시세를 네이버로부터 읽어서 DB에 업데이트"""
        # self.codes 딕셔너리에 저장된 모든 종목코드에 대해 순회처리한다. 
        for idx, code in enumerate(self.codes):
            # read_naver() 메서드를 이용하여 종목코드에 대한 일별 시세 데이터 프레임을 구한다.
            df = self.read_naver(code, self.codes[code], pages_to_fetch)
            if df is None:
                continue
            # 일별 시세 데이터프레임이 구해지면 replace_into_db() 메서드로 db에 저장한다.
            self.replace_into_db(df, idx, code, self.codes[code])

    def execute_daily(self):
        """실행 즉시 및 매일 오후 다섯시에 daily_price 데이블 업데이트"""
        self.update_comp_info()
        try:
            with open('config.json', 'r') as in_file:
                config = json.load(in_file)
                pages_to_fetch = config['pages_to_fetch']
        except FileExistsError:
            with open('config.json', 'w') as out_file:
                pages_to_fetch = 100
                config = {'pages_to_fetch' : 1}
                json.dump(config, out_file)
        self.update_daily_price(pages_to_fetch)

        tmnow = datetime.now()
        lastday = calendar.monthrange(tmnow.year, tmnow.month)[1]
        if tmnow.month == 12 and tmnow.day == lastday:
            tmnext = tmnow.replace(year=tmnow.year+1, month=1, day=1,
                hour=17, minute=0, second=0)
        elif tmnow.day == lastday:
            tmnext = tmnow.replace(month=tmnow.month+1, day=1, hour=17,
                minute=0, second=0)
        else:
            tmnext = tmnow.replace(day=tmnow.day+1, hour=17, minute=0,
                second=0)   
        tmdiff = tmnext - tmnow
        secs = tmdiff.seconds
        
        t = Timer(secs, self.execute_daily)
        print("Waiting for next update ({}) ... ".format(tmnext.strftime
            ('%Y-%m-%d %H:%M')))
        t.start()

if __name__ == '__main__':
    dbu = DBUpdater()
    dbu.execute_daily()
# DBUpdater.py가 단독으로 실행되면 DBUpdater 객체를 생성한다. 
# DBUpdater의 생성자 내부에서 마리아디비에 연결한다.
# company_info 테이블에 오늘 업데이트된 내용이 있는지 확인하고, 없으면 (4) 를 호출하여 compay_info 테이블에 업데이트하고 codes 딕셔너리에도 저장한다.
# KRX로부터 상장법인 목록 파일을 읽어온다.

